{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/miniconda3/envs/deepdebugger/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET resnet18\n",
      "Finish initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 432.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET resnet18_with_dropout\n",
      "Finish initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 6298.09it/s]\n"
     ]
    }
   ],
   "source": [
    "####### dropout resnet18 vs without dropout\n",
    "#### \n",
    "import torch\n",
    "import sys\n",
    "# sys.path.append(\"..\")\n",
    "sys.path.append(\"/home/yifan/projects/cophi/ContraVis\")\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"/home/yifan/projects/deepdebugertool/DLVisDebugger\")\n",
    "# REF_PATH = \"/home/yifan/Exp/Noise/0.2/experiment_vis\"\n",
    "# CONFIG_PATH = \"/home/yifan/experiments/noise/20\"\n",
    "# TAR_PATH = \"/home/yifan/dataset/cleanfornoise10\"\n",
    "\n",
    "# # CLEAN_PATH = \"/home/yifan/Exp/Noise/0.2/experiment1\"\n",
    "TAR_PATH = \"/home/yifan/dataset/resnet18_with_dropout/pairflip/cifar10/0\"\n",
    "REF_PATH = \"/home/yifan/dataset/clean/pairflip/cifar10/0\"\n",
    "\n",
    "\n",
    "\n",
    "ENCODER_DIMS=[512,256,256,256,256,2]\n",
    "DECODER_DIMS= [2,256,256,256,256,512]\n",
    "VIS_MODEL_NAME = 'vis2'\n",
    "\n",
    "########## initulize reference data and target data\n",
    "from singleVis.DataInit import DataInit\n",
    "REF_EPOCH = 200\n",
    "TAR_EPOCH = 200\n",
    "DEVICE = \"cuda:1\"\n",
    "tar_datainit = DataInit(TAR_PATH,TAR_PATH,TAR_EPOCH,DEVICE)\n",
    "ref_datainit = DataInit(REF_PATH,REF_PATH,REF_EPOCH,DEVICE)\n",
    "\n",
    "ref_model, ref_provider, ref_train_data, ref_prediction, ref_prediction_res, ref_scores = ref_datainit.getData()\n",
    "tar_model, tar_provider, tar_train_data, tar_prediction, tar_prediction_res, tar_scores = tar_datainit.getData()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 512)\n",
      "(50000, 512)\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import svd  \n",
    "def orthogonal_mapping(data1, data2):\n",
    "        \"\"\"\n",
    "        use Orthogonal Mapping, map data2 to data1's space。\n",
    "    \n",
    "        :param data1: numpy array, shape (n_samples, n_features)\n",
    "        :param data2: numpy array, shape (n_samples, n_features)\n",
    "        :return: data2_mapped: numpy array, mapped data2\n",
    "        \"\"\"\n",
    "        # step1: Centralized data\n",
    "        data1_centered = data1 - np.mean(data1, axis=0)\n",
    "        data2_centered = data2 - np.mean(data2, axis=0)\n",
    "        # step2: Calculate the cross-covariance matrix\n",
    "        C = data2_centered.T @ data1_centered\n",
    "        # step3: singular value decomposition\n",
    "        U, _, Vt = svd(C)\n",
    "\n",
    "        # step4: Compute orthogonal mapping matrix\n",
    "        W = U @ Vt\n",
    "\n",
    "        # step5: Apply mapping matrix\n",
    "        data2_mapped = data2_centered @ W\n",
    "    \n",
    "        return data2_mapped\n",
    "X_train1 = ref_provider.train_representation(REF_EPOCH)\n",
    "X_train2 = tar_provider.train_representation(TAR_EPOCH)\n",
    "X_train2 = X_train2.reshape(X_train2.shape[0],X_train2.shape[1])\n",
    "print(X_train1.shape)\n",
    "print(X_train2.shape)\n",
    "data2_mapped = orthogonal_mapping(X_train1,X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999986666666667\n"
     ]
    }
   ],
   "source": [
    "from eval.evaluate import *\n",
    "rate = evaluate_high_dimesion_trans_knn_preserving(data2_mapped, X_train2)\n",
    "print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.4547\n",
      "Epoch [2/200], Loss: 1.3728\n",
      "Epoch [3/200], Loss: 1.2530\n",
      "Epoch [4/200], Loss: 1.1483\n",
      "Epoch [5/200], Loss: 1.1644\n",
      "Epoch [6/200], Loss: 1.0922\n",
      "Epoch [7/200], Loss: 1.0067\n",
      "Epoch [8/200], Loss: 0.9752\n",
      "Epoch [9/200], Loss: 0.9610\n",
      "Epoch [10/200], Loss: 0.9349\n",
      "Epoch [11/200], Loss: 0.9039\n",
      "Epoch [12/200], Loss: 0.8864\n",
      "Epoch [13/200], Loss: 0.8732\n",
      "Epoch [14/200], Loss: 0.8514\n",
      "Epoch [15/200], Loss: 0.8357\n",
      "Epoch [16/200], Loss: 0.8287\n",
      "Epoch [17/200], Loss: 0.8180\n",
      "Epoch [18/200], Loss: 0.8037\n",
      "Epoch [19/200], Loss: 0.7955\n",
      "Epoch [20/200], Loss: 0.7897\n",
      "Epoch [21/200], Loss: 0.7789\n",
      "Epoch [22/200], Loss: 0.7694\n",
      "Epoch [23/200], Loss: 0.7631\n",
      "Epoch [24/200], Loss: 0.7549\n",
      "Epoch [25/200], Loss: 0.7468\n",
      "Epoch [26/200], Loss: 0.7401\n",
      "Epoch [27/200], Loss: 0.7308\n",
      "Epoch [28/200], Loss: 0.7211\n",
      "Epoch [29/200], Loss: 0.7135\n",
      "Epoch [30/200], Loss: 0.7052\n",
      "Epoch [31/200], Loss: 0.6967\n",
      "Epoch [32/200], Loss: 0.6888\n",
      "Epoch [33/200], Loss: 0.6799\n",
      "Epoch [34/200], Loss: 0.6708\n",
      "Epoch [35/200], Loss: 0.6624\n",
      "Epoch [36/200], Loss: 0.6531\n",
      "Epoch [37/200], Loss: 0.6437\n",
      "Epoch [38/200], Loss: 0.6343\n",
      "Epoch [39/200], Loss: 0.6245\n",
      "Epoch [40/200], Loss: 0.6149\n",
      "Epoch [41/200], Loss: 0.6054\n",
      "Epoch [42/200], Loss: 0.5955\n",
      "Epoch [43/200], Loss: 0.5857\n",
      "Epoch [44/200], Loss: 0.5755\n",
      "Epoch [45/200], Loss: 0.5649\n",
      "Epoch [46/200], Loss: 0.5544\n",
      "Epoch [47/200], Loss: 0.5435\n",
      "Epoch [48/200], Loss: 0.5324\n",
      "Epoch [49/200], Loss: 0.5210\n",
      "Epoch [50/200], Loss: 0.5093\n",
      "Epoch [51/200], Loss: 0.4975\n",
      "Epoch [52/200], Loss: 0.4858\n",
      "Epoch [53/200], Loss: 0.4741\n",
      "Epoch [54/200], Loss: 0.4626\n",
      "Epoch [55/200], Loss: 0.4512\n",
      "Epoch [56/200], Loss: 0.4400\n",
      "Epoch [57/200], Loss: 0.4290\n",
      "Epoch [58/200], Loss: 0.4182\n",
      "Epoch [59/200], Loss: 0.4075\n",
      "Epoch [60/200], Loss: 0.3970\n",
      "Epoch [61/200], Loss: 0.3867\n",
      "Epoch [62/200], Loss: 0.3768\n",
      "Epoch [63/200], Loss: 0.3672\n",
      "Epoch [64/200], Loss: 0.3579\n",
      "Epoch [65/200], Loss: 0.3489\n",
      "Epoch [66/200], Loss: 0.3402\n",
      "Epoch [67/200], Loss: 0.3319\n",
      "Epoch [68/200], Loss: 0.3239\n",
      "Epoch [69/200], Loss: 0.3160\n",
      "Epoch [70/200], Loss: 0.3084\n",
      "Epoch [71/200], Loss: 0.3010\n",
      "Epoch [72/200], Loss: 0.2939\n",
      "Epoch [73/200], Loss: 0.2872\n",
      "Epoch [74/200], Loss: 0.2809\n",
      "Epoch [75/200], Loss: 0.2749\n",
      "Epoch [76/200], Loss: 0.2694\n",
      "Epoch [77/200], Loss: 0.2644\n",
      "Epoch [78/200], Loss: 0.2597\n",
      "Epoch [79/200], Loss: 0.2553\n",
      "Epoch [80/200], Loss: 0.2513\n",
      "Epoch [81/200], Loss: 0.2476\n",
      "Epoch [82/200], Loss: 0.2441\n",
      "Epoch [83/200], Loss: 0.2408\n",
      "Epoch [84/200], Loss: 0.2377\n",
      "Epoch [85/200], Loss: 0.2349\n",
      "Epoch [86/200], Loss: 0.2323\n",
      "Epoch [87/200], Loss: 0.2299\n",
      "Epoch [88/200], Loss: 0.2278\n",
      "Epoch [89/200], Loss: 0.2258\n",
      "Epoch [90/200], Loss: 0.2239\n",
      "Epoch [91/200], Loss: 0.2222\n",
      "Epoch [92/200], Loss: 0.2207\n",
      "Epoch [93/200], Loss: 0.2193\n",
      "Epoch [94/200], Loss: 0.2180\n",
      "Epoch [95/200], Loss: 0.2168\n",
      "Epoch [96/200], Loss: 0.2157\n",
      "Epoch [97/200], Loss: 0.2147\n",
      "Epoch [98/200], Loss: 0.2137\n",
      "Epoch [99/200], Loss: 0.2129\n",
      "Epoch [100/200], Loss: 0.2122\n",
      "Epoch [101/200], Loss: 0.2115\n",
      "Epoch [102/200], Loss: 0.2110\n",
      "Epoch [103/200], Loss: 0.2108\n",
      "Epoch [104/200], Loss: 0.2115\n",
      "Epoch [105/200], Loss: 0.2131\n",
      "Epoch [106/200], Loss: 0.2126\n",
      "Epoch [107/200], Loss: 0.2090\n",
      "Epoch [108/200], Loss: 0.2085\n",
      "Epoch [109/200], Loss: 0.2101\n",
      "Epoch [110/200], Loss: 0.2086\n",
      "Epoch [111/200], Loss: 0.2070\n",
      "Epoch [112/200], Loss: 0.2081\n",
      "Epoch [113/200], Loss: 0.2073\n",
      "Epoch [114/200], Loss: 0.2060\n",
      "Epoch [115/200], Loss: 0.2066\n",
      "Epoch [116/200], Loss: 0.2063\n",
      "Epoch [117/200], Loss: 0.2052\n",
      "Epoch [118/200], Loss: 0.2056\n",
      "Epoch [119/200], Loss: 0.2054\n",
      "Epoch [120/200], Loss: 0.2046\n",
      "Epoch [121/200], Loss: 0.2047\n",
      "Epoch [122/200], Loss: 0.2046\n",
      "Epoch [123/200], Loss: 0.2039\n",
      "Epoch [124/200], Loss: 0.2039\n",
      "Epoch [125/200], Loss: 0.2039\n",
      "Epoch [126/200], Loss: 0.2035\n",
      "Epoch [127/200], Loss: 0.2033\n",
      "Epoch [128/200], Loss: 0.2033\n",
      "Epoch [129/200], Loss: 0.2031\n",
      "Epoch [130/200], Loss: 0.2028\n",
      "Epoch [131/200], Loss: 0.2027\n",
      "Epoch [132/200], Loss: 0.2027\n",
      "Epoch [133/200], Loss: 0.2024\n",
      "Epoch [134/200], Loss: 0.2022\n",
      "Epoch [135/200], Loss: 0.2022\n",
      "Epoch [136/200], Loss: 0.2021\n",
      "Epoch [137/200], Loss: 0.2019\n",
      "Epoch [138/200], Loss: 0.2018\n",
      "Epoch [139/200], Loss: 0.2017\n",
      "Epoch [140/200], Loss: 0.2016\n",
      "Epoch [141/200], Loss: 0.2014\n",
      "Epoch [142/200], Loss: 0.2013\n",
      "Epoch [143/200], Loss: 0.2013\n",
      "Epoch [144/200], Loss: 0.2012\n",
      "Epoch [145/200], Loss: 0.2010\n",
      "Epoch [146/200], Loss: 0.2010\n",
      "Epoch [147/200], Loss: 0.2010\n",
      "Epoch [148/200], Loss: 0.2012\n",
      "Epoch [149/200], Loss: 0.2016\n",
      "Epoch [150/200], Loss: 0.2026\n",
      "Epoch [151/200], Loss: 0.2040\n",
      "Epoch [152/200], Loss: 0.2043\n",
      "Epoch [153/200], Loss: 0.2027\n",
      "Epoch [154/200], Loss: 0.2010\n",
      "Epoch [155/200], Loss: 0.2015\n",
      "Epoch [156/200], Loss: 0.2021\n",
      "Epoch [157/200], Loss: 0.2010\n",
      "Epoch [158/200], Loss: 0.2003\n",
      "Epoch [159/200], Loss: 0.2012\n",
      "Epoch [160/200], Loss: 0.2012\n",
      "Epoch [161/200], Loss: 0.2001\n",
      "Epoch [162/200], Loss: 0.1999\n",
      "Epoch [163/200], Loss: 0.2005\n",
      "Epoch [164/200], Loss: 0.2002\n",
      "Epoch [165/200], Loss: 0.1997\n",
      "Epoch [166/200], Loss: 0.2000\n",
      "Epoch [167/200], Loss: 0.2000\n",
      "Epoch [168/200], Loss: 0.1994\n",
      "Epoch [169/200], Loss: 0.1994\n",
      "Epoch [170/200], Loss: 0.1997\n",
      "Epoch [171/200], Loss: 0.1994\n",
      "Epoch [172/200], Loss: 0.1992\n",
      "Epoch [173/200], Loss: 0.1994\n",
      "Epoch [174/200], Loss: 0.1993\n",
      "Epoch [175/200], Loss: 0.1990\n",
      "Epoch [176/200], Loss: 0.1990\n",
      "Epoch [177/200], Loss: 0.1990\n",
      "Epoch [178/200], Loss: 0.1989\n",
      "Epoch [179/200], Loss: 0.1987\n",
      "Epoch [180/200], Loss: 0.1987\n",
      "Epoch [181/200], Loss: 0.1987\n",
      "Epoch [182/200], Loss: 0.1986\n",
      "Epoch [183/200], Loss: 0.1985\n",
      "Epoch [184/200], Loss: 0.1985\n",
      "Epoch [185/200], Loss: 0.1984\n",
      "Epoch [186/200], Loss: 0.1983\n",
      "Epoch [187/200], Loss: 0.1983\n",
      "Epoch [188/200], Loss: 0.1984\n",
      "Epoch [189/200], Loss: 0.1984\n",
      "Epoch [190/200], Loss: 0.1985\n",
      "Epoch [191/200], Loss: 0.1989\n",
      "Epoch [192/200], Loss: 0.1998\n",
      "Epoch [193/200], Loss: 0.2016\n",
      "Epoch [194/200], Loss: 0.2041\n",
      "Epoch [195/200], Loss: 0.2051\n",
      "Epoch [196/200], Loss: 0.2020\n",
      "Epoch [197/200], Loss: 0.1982\n",
      "Epoch [198/200], Loss: 0.1991\n",
      "Epoch [199/200], Loss: 0.2017\n",
      "Epoch [200/200], Loss: 0.2003\n"
     ]
    }
   ],
   "source": [
    "from contrast.transfomration import *\n",
    "tarns_model,tar_mapped,ref_reconstructed  = transformation_train(X_train1,X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 7580.09it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 6600.38it/s]\n"
     ]
    }
   ],
   "source": [
    "ref_pred = ref_provider.get_pred(REF_EPOCH, X_train1).argmax(axis=1)\n",
    "tar_pred = tar_provider.get_pred(TAR_EPOCH, X_train2).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5552391 , 1.3357137 , 0.        , ..., 1.4515507 , 1.2634635 ,\n",
       "        0.        ],\n",
       "       [1.4458725 , 0.30887666, 2.0563803 , ..., 0.        , 0.97767997,\n",
       "        0.        ],\n",
       "       [1.4317726 , 0.17496452, 1.501822  , ..., 0.        , 1.0427316 ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [1.1769273 , 0.        , 1.1185749 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.1797391 , 0.13735478, 1.4692004 , ..., 0.20582874, 0.6571957 ,\n",
       "        2.3002887 ],\n",
       "       [0.84440976, 0.2670025 , 1.9620161 , ..., 0.        , 1.078705  ,\n",
       "        1.0086372 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500748\n"
     ]
    }
   ],
   "source": [
    "from eval.evaluate import *\n",
    "rate = evaluate_high_dimesion_trans_knn_preserving(tar_mapped, X_train2)\n",
    "print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 6842.17it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reconstructed_pred = tar_provider.get_pred(TAR_EPOCH, ref_reconstructed).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "diff = 0\n",
    "for i in range(len(reconstructed_pred)):\n",
    "    if reconstructed_pred[i] != tar_pred[i]:\n",
    "        diff = diff+1\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff pred number is 41\n",
      "index diff pred number is 12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "sub_len = 50000\n",
    "\n",
    "# 1. use cos sim build \n",
    "model = NearestNeighbors(n_neighbors=1, metric='cosine', algorithm='auto').fit(tar_mapped[:sub_len])\n",
    "\n",
    "# 2. find X_train1 most sim point data2_mapped中的最相似点\n",
    "distances, most_similar_points = model.kneighbors(X_train1[:sub_len])\n",
    "\n",
    "most_similar_points = most_similar_points.flatten()  # 将索引数组转为1D\n",
    "\n",
    "# 3. 比较预测值\n",
    "m = np.sum(ref_pred[:sub_len] != tar_pred[most_similar_points])\n",
    "k = np.sum(ref_pred[:sub_len] != tar_pred[:sub_len])\n",
    "\n",
    "print(\"diff pred number is {}\".format(m))\n",
    "print(\"index diff pred number is {}\".format(k))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdebugger",
   "language": "python",
   "name": "deepdebugger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
