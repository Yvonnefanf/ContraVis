{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/miniconda3/envs/deepdebugger/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-30 21:39:05.470235: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 21:39:06.058464: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 21:39:06.058512: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 21:39:06.058517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish initialization...\n",
      "Finish initialization...\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "#                                                          IMPORT                                                      #\n",
    "########################################################################################################################\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "sys.path.append(\"/home/yifan/projects/cophi/ContraVis\")\n",
    "\n",
    "from umap.umap_ import find_ab_params\n",
    "from contrast.transfomration import *\n",
    "\n",
    "from singleVis.SingleVisualizationModel import VisModel\n",
    "from singleVis.losses import UmapLoss, ReconstructionLoss, SingleVisLoss\n",
    "\n",
    "from singleVis.data import NormalDataProvider\n",
    "\n",
    "\n",
    "from singleVis.projector import DVIProjector,ContraProjector\n",
    "from singleVis.utils import find_neighbor_preserving_rate\n",
    "\n",
    "########################################################################################################################\n",
    "#                                                      PARAMETERS                                                   #\n",
    "########################################################################################################################\n",
    "\"\"\"This serve as an example of DeepVisualInsight implementation in pytorch.\"\"\"\n",
    "VIS_METHOD = \"DVI\" # DeepVisualInsight\n",
    "\n",
    "\n",
    "TAR_PATH = \"/home/yifan/experiments/backdoor/resnet18_CIFAR10/experiment10\"\n",
    "REF_PATH = \"/home/yifan/dataset/clean/pairflip/cifar10/0\"\n",
    "\n",
    "########################################################################################################################\n",
    "#                                                     LOAD PARAMETERS                                                  #\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "CONTENT_PATH = REF_PATH\n",
    "sys.path.append(CONTENT_PATH)\n",
    "with open(os.path.join(CONTENT_PATH, \"config.json\"), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config = config[VIS_METHOD]\n",
    "\n",
    "# record output information\n",
    "# now = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time())) \n",
    "# sys.stdout = open(os.path.join(CONTENT_PATH, now+\".txt\"), \"w\")\n",
    "\n",
    "SETTING = config[\"SETTING\"]\n",
    "CLASSES = config[\"CLASSES\"]\n",
    "DATASET = config[\"DATASET\"]\n",
    "PREPROCESS = config[\"VISUALIZATION\"][\"PREPROCESS\"]\n",
    "GPU_ID = config[\"GPU\"]\n",
    "GPU_ID = 1\n",
    "EPOCH_START = config[\"EPOCH_START\"]\n",
    "EPOCH_END = config[\"EPOCH_END\"]\n",
    "EPOCH_PERIOD = config[\"EPOCH_PERIOD\"]\n",
    "\n",
    "EPOCH_START = 200\n",
    "EPOCH_END = 200\n",
    "EPOCH_PERIOD = 1\n",
    "\n",
    "# Training parameter (subject model)\n",
    "TRAINING_PARAMETER = config[\"TRAINING\"]\n",
    "NET = TRAINING_PARAMETER[\"NET\"]\n",
    "LEN = TRAINING_PARAMETER[\"train_num\"]\n",
    "\n",
    "# Training parameter (visualization model)\n",
    "VISUALIZATION_PARAMETER = config[\"VISUALIZATION\"]\n",
    "LAMBDA1 = VISUALIZATION_PARAMETER[\"LAMBDA1\"]\n",
    "LAMBDA2 = VISUALIZATION_PARAMETER[\"LAMBDA2\"]\n",
    "B_N_EPOCHS = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"B_N_EPOCHS\"]\n",
    "L_BOUND = VISUALIZATION_PARAMETER[\"BOUNDARY\"][\"L_BOUND\"]\n",
    "ENCODER_DIMS = VISUALIZATION_PARAMETER[\"ENCODER_DIMS\"]\n",
    "DECODER_DIMS = VISUALIZATION_PARAMETER[\"DECODER_DIMS\"]\n",
    "\n",
    "\n",
    "S_N_EPOCHS = VISUALIZATION_PARAMETER[\"S_N_EPOCHS\"]\n",
    "N_NEIGHBORS = VISUALIZATION_PARAMETER[\"N_NEIGHBORS\"]\n",
    "PATIENT = VISUALIZATION_PARAMETER[\"PATIENT\"]\n",
    "MAX_EPOCH = VISUALIZATION_PARAMETER[\"MAX_EPOCH\"]\n",
    "\n",
    "VIS_MODEL_NAME = 'Contravis_backdoor' ### saved_as \n",
    "EVALUATION_NAME = VISUALIZATION_PARAMETER[\"EVALUATION_NAME\"]\n",
    "\n",
    "# Define hyperparameters\n",
    "DEVICE = torch.device(\"cuda:{}\".format(GPU_ID) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import Model.model as subject_model\n",
    "net = eval(\"subject_model.{}()\".format(NET))\n",
    "\n",
    "########################################################################################################################\n",
    "#                                                    TRAINING SETTING                                                  #\n",
    "########################################################################################################################\n",
    "# Define data_provider\n",
    "#TODO\n",
    "TAE_NET = \"resnet18\"\n",
    "tar_net = eval(\"subject_model.{}()\".format(TAE_NET)) \n",
    "\n",
    "data_provider = NormalDataProvider(CONTENT_PATH, net, EPOCH_START, EPOCH_END, EPOCH_PERIOD, device=DEVICE, epoch_name='Epoch',classes=CLASSES,verbose=1)\n",
    "tar_data_provider = NormalDataProvider(TAR_PATH, tar_net, EPOCH_START, EPOCH_END, EPOCH_PERIOD, device=DEVICE, epoch_name='Epoch',classes=CLASSES,verbose=1)\n",
    "\n",
    "\n",
    "# Define visualization models\n",
    "model = VisModel(ENCODER_DIMS, DECODER_DIMS)\n",
    "\n",
    "\n",
    "# Define Losses\n",
    "negative_sample_rate = 5\n",
    "min_dist = .1\n",
    "_a, _b = find_ab_params(1.0, min_dist)\n",
    "umap_loss_fn = UmapLoss(negative_sample_rate, DEVICE, _a, _b, repulsion_strength=1.0)\n",
    "recon_loss_fn = ReconstructionLoss(beta=1.0)\n",
    "single_loss_fn = SingleVisLoss(umap_loss_fn, recon_loss_fn, lambd=LAMBDA1)\n",
    "# Define Projector\n",
    "projector = ContraProjector(vis_model=model, content_path=os.path.join(REF_PATH, 'Model', 'Epoch_{}//Contravis.pth'.format(EPOCH_START)), vis_model_name=VIS_MODEL_NAME, device=DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 460.11it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 7583.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start calculating the k nearest neibour...\n",
      "start calculating the k nearest neibour...\n"
     ]
    }
   ],
   "source": [
    "from semantic_distance_calculator import SemanticDistanceCalculator\n",
    "\n",
    "ss = SemanticDistanceCalculator(data_provider,tar_data_provider,200,200,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_most_similar_ref_for_each_tar(num=50):\n",
    "    max_similarity_indices = np.zeros(num, dtype=int)  # 存储最高相似度的参考点索引\n",
    "    max_similarity_scores = np.zeros(num)  # 存储每个目标点的最高相似度分数\n",
    "\n",
    "    for t_index in range(50000):\n",
    "        highest_similarity = -np.inf  # 初始化为负无穷大\n",
    "        highest_similarity_r_index = -1  # 初始化索引\n",
    "\n",
    "        for r_index in range(50000):\n",
    "            # 计算相似度分数\n",
    "            similarity, _, _ = ss.tar_ref_train_data_semantic_similairty_(r_index, t_index)\n",
    "\n",
    "            # 检查是否为最高分数\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                highest_similarity_r_index = r_index\n",
    "\n",
    "        max_similarity_indices[t_index] = highest_similarity_r_index\n",
    "        max_similarity_scores[t_index] = highest_similarity\n",
    "\n",
    "        # 可以在这里打印进度\n",
    "        if t_index % 1000 == 0:\n",
    "            print(f\"Processed {t_index} target points...\")\n",
    "\n",
    "    return max_similarity_indices, max_similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 target points...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2886184/2612555657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_most_similar_ref_for_each_tar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2886184/1734776321.py\u001b[0m in \u001b[0;36mfind_most_similar_ref_for_each_tar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# 计算相似度分数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0msimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtar_ref_train_data_semantic_similairty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# 检查是否为最高分数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/cophi/ContraVis/Semantic_similarity/semantic_distance_calculator.py\u001b[0m in \u001b[0;36mtar_ref_train_data_semantic_similairty_\u001b[0;34m(self, r_index, t_index)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# 示例用法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mref_weighted_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_knn_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_knn_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtar_weighted_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_knn_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_knn_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mrelative_cos_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_weighted_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_weighted_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/cophi/ContraVis/Semantic_similarity/semantic_distance_calculator.py\u001b[0m in \u001b[0;36mweighted_average\u001b[0;34m(self, preds, dists)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# change distance to weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# To prevent division by 0, add a small constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# calculate the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a, b = find_most_similar_ref_for_each_tar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_cos_similarity 0.84375876 cos_similiarty 0.8437565\n"
     ]
    }
   ],
   "source": [
    "ss.tar_ref_train_data_semantic_similairty_(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_cos_similarity 0.67687386 cos_similiarty 1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 7865.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.8521895  -8.175585   -7.513469    4.1612267   5.6394467   2.8419204\n",
      "  29.126104   -3.580028   -5.381106  -11.698126 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = data_provider.get_pred(200, data_provider.train_representation(200))\n",
    "print(pred1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 7682.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4185712  -1.4216353  -0.9698236   0.04930839 -0.51778585  0.324724\n",
      "  9.793958   -1.5141854  -1.3146505  -2.0116844 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = tar_data_provider.train_representation(200)\n",
    "train_data = train_data.reshape(len(train_data), -1)\n",
    "pred2 = tar_data_provider.get_pred(200, train_data)\n",
    "print(pred2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # cal_culate\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity:  0.9421209\n"
     ]
    }
   ],
   "source": [
    "# softmax_vec1 = softmax(pred1[10])\n",
    "# softmax_vec2 = softmax(pred2[10])\n",
    "softmax_vec1 = pred1[0]\n",
    "softmax_vec2 = pred2[0]\n",
    "cos_sim = cosine_similarity(softmax_vec1, softmax_vec2)\n",
    "print(\"Cosine Similarity: \", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity:  1.0\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdebugger",
   "language": "python",
   "name": "deepdebugger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
